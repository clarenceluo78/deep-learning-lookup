{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "## Graph Representation\n",
    "A graph $\\mathcal{G}$ is defiend as a tuple of a set of nodes/vertices $V$, and a set of edges/links $E$: $\\mathcal{G} = (V,E)$. Each edge is a pair of two vertices, and represents a connection between them. For instance, let’s look at the following graph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G= nx.Graph()\n",
    "G.add_nodes_from([(1,{'color':'orange'}),\n",
    "                  (2,{'color':'green'}),\n",
    "                  (3,{'color':'yellow'}),\n",
    "                  (4,{'color':'blue'})])# V = \\{1,2,3,4\\}\n",
    "G.add_edges_from([(1,2),(2,3),(2,4),(3,4)]) # E = \\{(1,2),(2,3),(2,4),(3,4)\\}\n",
    "colors = [data['color'] for node, data in G.nodes(data=True)]\n",
    "fig, ax = plt.subplots(figsize=(3, 2))\n",
    "nx.draw(G, with_labels=True,node_color = colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In application, vertices and edge can often have specific ***attributes***, and edges can even be directed. The question is how we could represent this diversity in an efficient way for matrix operations.\n",
    "Usually, for the edges, we my represent as an ***adjacency matrix***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjacency Matrix\n",
    "The adjacency matrix $A$ is a square matrix whose elements indicate whether pairs of vertices are adjacent, i.e. connected, or not. In the simplest case\n",
    "$$\n",
    "A_{ij} = \\left\\{\\begin{aligned}&1, \\text{ if there is a connection from node i to j};\\\\\n",
    "&0, \\text{ otherwise.} \\end{aligned}\\right.\n",
    "$$\n",
    "For an undirected graph, $A$ is a symmetric matrix($A_{ij}=A_{ji}$).\n",
    "For the above example, we have \n",
    "\n",
    "$$\n",
    "A = \\left[\\begin{matrix}\n",
    "0&1&0&0\\\\\n",
    "1&0&1&1\\\\\n",
    "0&1&0&1\\\\\n",
    "0&1&1&0\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "If we have edge attributes or different categories of edges in a graph, this information can be added to the matrix as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution Networks(GCNs)\n",
    "Input: node features $ \\mathbf{x}_i$ for node $i \\in V$, adjacency matrix $A$.\n",
    "Now, we study the graph convolutional layer from scratch.\n",
    "\n",
    "In neural networks, linear layers apply a linear transformation to the incoming data. They transform input features $\\mathbf{x}$ into hidden vectors $\\mathbf{h}$ using a weight matrix $W$. If we ignore biases, we can write:\n",
    "$$\n",
    "\\mathbf{h} = W\\mathbf{x}\n",
    "$$\n",
    "\n",
    "With graph data, we have access to ***connections between nodes.*** Under a hypothesis that similar nodes are more likely to be connected to each other than dissimilar ones(network homophily), we can enrich the node representation by ***aggregating its features with those of its neighbors***, a.k.a. convolution or neighborhood aggregation:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i = \\sum_{j \\in \\tilde{\\mathcal{N}}_i}W\\mathbf{x}_j,\n",
    "$$\n",
    "\n",
    "where $\\tilde{\\mathcal{N}}_i$ is the neighborhood of node $i$ including itself.\n",
    "\n",
    "Unlike filters in Convolutional Neural Networks (CNNs), our weight matrix $W$ is unique and shared among every node. But there is another issue: nodes do not have a fixed number of neighbors like pixels do.\n",
    "\n",
    "To address this issue, we can normalize the result based on the number of connections. In graph theory, this number is called a degree.\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i = \\frac{1}{\\deg(i)}\\sum_{j \\in \\tilde{\\mathcal{N}}_i}W\\mathbf{x}_j\n",
    "$$\n",
    "\n",
    "However, the above normalization can be ill-conditioned since the authors of GCN noticed that features from nodes with a lot of neighbors will spread much more easily than those from more isolated nodes. To counterbalance this effect, they proposed to give bigger weights to features from nodes with few neighbors. This operation can be written as follows:\n",
    "\n",
    "$$\n",
    "h_i = \\sum_{j \\in \\tilde{\\mathcal{N}}_i} \\dfrac{1}{\\sqrt{\\deg(i)}\\sqrt{\\deg(j)}} \\mathbf{W} x_j\n",
    "$$\n",
    "\n",
    "GCN:\n",
    "\n",
    "$$\n",
    "\\mathbf{H}^{(l+1)} = \\sigma(\\hat{\\mathbf{A}}\\mathbf{H}^{(l)}\\mathbf{W}^{(l)}),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{H}^{(l)}$ is feature embedding matrix in the $l-th$ layer;\n",
    "\n",
    "$\\sigma$ is an activation function;\n",
    "\n",
    "$W^{(l)}$ is parameter matrix;\n",
    "\n",
    "$\\hat{\\mathbf{A}} = \\tilde{\\mathbf{D}}^{-\\frac{1}{2}}\\tilde{\\mathbf{A}}\\tilde{\\mathbf{D}}^{-\\frac{1}{2}}, \\tilde{\\mathbf{A}} = A+I,\\tilde{\\mathbf{D}}^{-\\frac{1}{2}} = diag(\\sum_i\\tilde{\\mathbf{A}}_{i1},\\cdots,\\sum_i\\tilde{\\mathbf{A}}_{in})$;\n",
    "\n",
    "$\\mathbf{H}^{(0)} = \\mathbf{X}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# A simplified version of GCN\n",
    "class GCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
    "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections.\n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # Num neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features:\n",
      " tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\n",
      "Adjacency matrix:\n",
      " tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# Apply GCN to our example graph\n",
    "# pecify some node features and the adjacency matrix with added self-connections:\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
    "                            [1, 1, 1, 1],\n",
    "                            [0, 1, 1, 1],\n",
    "                            [0, 1, 1, 1]]])\n",
    "\n",
    "print(\"Node features:\\n\", node_feats)\n",
    "print(\"\\nAdjacency matrix:\\n\", adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[1., 2.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "# initialize the linear weight matrix as an identity matrix\n",
    "layer = GCNLayer(c_in=2, c_out=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix)\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue we can see from looking at the example above is that the output features for nodes 3 and 4 are the same because they have the same adjacent nodes (including itself). Therefore, GCN layers can make the network forget node-specific information if we just take a mean over all messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks on graph-structured data\n",
    "### Node-level tasks: Semi-supervised node classification\n",
    "Goal: Classify nodes in a graph.\n",
    "Input: Usually a single, large graph with >1000 nodes of which a certain amount of nodes are labeled.\n",
    "\n",
    "We learn to classify those labeled examples during training and try to generalize to the unlabeled nodes.\n",
    "\n",
    "***Example:*** Cora dataset is a citation network among papers. The Cora consists of 2708 scientific publications with links between each other representing the citation of one paper by another. The task is to classify each publication into one of seven classes. Each publication is represented by a bag-of-words vector. This means that we have a vector of 1433 elements for each publication, where a $1$ at feature $i$ indicates that the $i-th$ word of a pre-defined dictionary is in the article.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import torch_geometric.nn as geom_nn\n",
    "import torch_geometric.data as geom_data\n",
    "# load data\n",
    "cora_dataset = torch_geometric.datasets.Planetoid(root='/Users/zoeyzhu/Desktop/', name=\"Cora\")\n",
    "CHECKPOINT_PATH = \"/Users/zoeyzhu/Desktop/saved_models/gnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_layer_by_name = {\n",
    "    \"GCN\": geom_nn.GCNConv,\n",
    "    \"GAT\": geom_nn.GATConv,\n",
    "    \"GraphConv\": geom_nn.GraphConv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers - Number of \"hidden\" graph layers\n",
    "            layer_name - String of the graph layer to use\n",
    "            dp_rate - Dropout rate to apply throughout the network\n",
    "            kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        gnn_layer = gnn_layer_by_name[layer_name]\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          **kwargs),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [gnn_layer(in_channels=in_channels,\n",
    "                             out_channels=c_out,\n",
    "                             **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
    "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
    "            # we can simply check the class type.\n",
    "            if isinstance(l, geom_nn.MessagePassing):\n",
    "                x = l(x, edge_index)\n",
    "            else:\n",
    "                x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers - Number of hidden layers\n",
    "            dp_rate - Dropout rate to apply throughout the network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                nn.Linear(in_channels, out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [nn.Linear(in_channels, c_out)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeLevelGNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if model_name == \"MLP\":\n",
    "            self.model = MLPModel(**model_kwargs)\n",
    "        else:\n",
    "            self.model = GNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.model(x, edge_index)\n",
    "\n",
    "        # Only calculate the loss on the nodes corresponding to the mask\n",
    "        if mode == \"train\":\n",
    "            mask = data.train_mask\n",
    "        elif mode == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif mode == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            assert False, f\"Unknown forward mode: {mode}\"\n",
    "\n",
    "        loss = self.loss_module(x[mask], data.y[mask])\n",
    "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We use SGD here, but Adam works as well\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "In the single graph, use a batch size of 1 for the data loader and share the same data loader for the train, validation, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def train_node_classifier(model_name, dataset, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "    node_data_loader = geom_data.DataLoader(dataset, batch_size=1)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=200,\n",
    "                         enable_progress_bar=False) # False because epoch size is 1\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"NodeLevel{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = NodeLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything()\n",
    "        model = NodeLevelGNN(model_name=model_name, c_in=dataset.num_node_features, c_out=dataset.num_classes, **model_kwargs)\n",
    "        trainer.fit(model, node_data_loader, node_data_loader)\n",
    "        model = NodeLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on the test set\n",
    "    test_result = trainer.test(model, node_data_loader, verbose=False)\n",
    "    batch = next(iter(node_data_loader))\n",
    "    batch = batch.to(model.device)\n",
    "    _, train_acc = model.forward(batch, mode=\"train\")\n",
    "    _, val_acc = model.forward(batch, mode=\"val\")\n",
    "    result = {\"train\": train_acc,\n",
    "              \"val\": val_acc,\n",
    "              \"test\": test_result[0]['test_acc']}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small function for printing the test scores\n",
    "def print_results(result_dict):\n",
    "    if \"train\" in result_dict:\n",
    "        print(f\"Train accuracy: {(100.0*result_dict['train']):4.2f}%\")\n",
    "    if \"val\" in result_dict:\n",
    "        print(f\"Val accuracy:   {(100.0*result_dict['val']):4.2f}%\")\n",
    "    print(f\"Test accuracy:  {(100.0*result_dict['test']):4.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP for node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/Users/zoeyzhu/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/zoeyzhu/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Global seed set to 42\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | MLPModel         | 23.1 K\n",
      "1 | loss_module | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "23.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.1 K    Total params\n",
      "0.092     Total estimated model params size (MB)\n",
      "/Users/zoeyzhu/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/zoeyzhu/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2708. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/Users/zoeyzhu/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/zoeyzhu/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 100.00%\n",
      "Val accuracy:   51.80%\n",
      "Test accuracy:  58.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoeyzhu/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "node_mlp_model, node_mlp_result = train_node_classifier(model_name=\"MLP\",\n",
    "                                                        dataset=cora_dataset,\n",
    "                                                        c_hidden=16,\n",
    "                                                        num_layers=2,\n",
    "                                                        dp_rate=0.1)\n",
    "\n",
    "print_results(node_mlp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the MLP can ***overfit*** on the training dataset because of the high-dimensional input features, it does not perform too well on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 layer GCN for node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | GNNModel         | 23.1 K\n",
      "1 | loss_module | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "23.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.1 K    Total params\n",
      "0.092     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 99.29%\n",
      "Val accuracy:   76.20%\n",
      "Test accuracy:  81.40%\n"
     ]
    }
   ],
   "source": [
    "node_gnn_model, node_gnn_result = train_node_classifier(model_name=\"GNN\",\n",
    "                                                        layer_name=\"GCN\",\n",
    "                                                        dataset=cora_dataset,\n",
    "                                                        c_hidden=16,\n",
    "                                                        num_layers=2,\n",
    "                                                        dp_rate=0.1)\n",
    "print_results(node_gnn_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GNN model outperforms the MLP by quite a margin. This shows that using the graph information indeed improves our predictions and lets us generalizes better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph-level tasks: Graph classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Goal:*** To classify an entire graph instead of single nodes or edges.\n",
    "\n",
    "***Input:*** A dataset of multiple graphs that we need to classify based on some structural graph properties.\n",
    "\n",
    "The most common task for graph classification is ***molecular property prediction***, in which molecules are represented as graphs. Each atom is linked to a node, and edges in the graph are the bonds between atoms.\n",
    "\n",
    "The dataset we will use below is called the MUTAG dataset. It is a common small benchmark for graph classification algorithms, and contain ***188 graphs*** with 18 nodes and 20 edges on average for each graph. The graph nodes have 7 different labels/atom types, and the binary graph labels represent “their mutagenic effect on a specific gram negative bacterium” (the specific meaning of the labels are not too important here). \n",
    "\n",
    "We can load the dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu_dataset = torch_geometric.datasets.TUDataset(root='/Users/zoeyzhu/Desktop/', name=\"MUTAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(x=[3371, 7], edge_index=[2, 7442], edge_attr=[7442, 4], y=[188])\n",
      "Length: 188\n",
      "Average label: 0.66\n"
     ]
    }
   ],
   "source": [
    "print(\"Data object:\", tu_dataset.data)\n",
    "print(\"Length:\", len(tu_dataset))\n",
    "print(f\"Average label: {tu_dataset.data.y.float().mean().item():4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line shows how the dataset stores different graphs. The nodes, edges, and labels of each graph are concatenated to one tensor, and the dataset stores the indices where to split the tensors correspondingly. \n",
    "\n",
    "The length of the dataset is the number of graphs we have, and the “average label” denotes the percentage of the graph with label 1. As long as the percentage is in the range of 0.5, we have a relatively balanced dataset. \n",
    "\n",
    "Next, we split our dataset into a training and test part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "tu_dataset.shuffle()\n",
    "train_dataset = tu_dataset[:150]\n",
    "test_dataset = tu_dataset[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_train_loader = geom_data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=64) # Additional loader if you want to change to a larger dataset\n",
    "graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: DataBatch(edge_index=[2, 1512], x=[687, 7], edge_attr=[1512, 4], y=[38], batch=[687], ptr=[39])\n",
      "Labels: tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0])\n",
      "Batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(graph_test_loader))\n",
    "print(\"Batch:\", batch)\n",
    "print(\"Labels:\", batch.y[:10])\n",
    "print(\"Batch indices:\", batch.batch[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of output features (usually number of classes)\n",
    "            dp_rate_linear - Dropout rate before the linear layer (usually much higher than inside the GNN)\n",
    "            kwargs - Additional arguments for the GNNModel object\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.GNN = GNNModel(c_in=c_in,\n",
    "                            c_hidden=c_hidden,\n",
    "                            c_out=c_hidden, # Not our prediction output yet!\n",
    "                            **kwargs)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dp_rate_linear),\n",
    "            nn.Linear(c_hidden, c_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch_idx):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "            batch_idx - Index of batch element for each node\n",
    "        \"\"\"\n",
    "        x = self.GNN(x, edge_index)\n",
    "        x = geom_nn.global_mean_pool(x, batch_idx) # Average pooling\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLevelGNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = GraphGNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
    "        x = self.model(x, edge_index, batch_idx)\n",
    "        x = x.squeeze(dim=-1)\n",
    "\n",
    "        if self.hparams.c_out == 1:\n",
    "            preds = (x > 0).float()\n",
    "            data.y = data.y.float()\n",
    "        else:\n",
    "            preds = x.argmax(dim=-1)\n",
    "        loss = self.loss_module(x, data.y)\n",
    "        acc = (preds == data.y).sum().float() / preds.shape[0]\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0) # High lr because of small dataset and small model\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph_classifier(model_name, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=500,\n",
    "                         enable_progress_bar=False)\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"GraphLevel{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model = GraphLevelGNN(c_in=tu_dataset.num_node_features,\n",
    "                              c_out=1 if tu_dataset.num_classes==2 else tu_dataset.num_classes,\n",
    "                              **model_kwargs)\n",
    "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
    "        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    # Test best model on validation and test set\n",
    "    train_result = trainer.test(model, graph_train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, graph_test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0]['test_acc'], \"train\": train_result[0]['test_acc']}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "\n",
      "  | Name        | Type              | Params\n",
      "--------------------------------------------------\n",
      "0 | model       | GraphGNNModel     | 266 K \n",
      "1 | loss_module | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------------\n",
      "266 K     Trainable params\n",
      "0         Non-trainable params\n",
      "266 K     Total params\n",
      "1.067     Total estimated model params size (MB)\n",
      "/Users/zoeyzhu/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/Users/zoeyzhu/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "/Users/zoeyzhu/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "model, result = train_graph_classifier(model_name=\"GraphConv\",\n",
    "                                       c_hidden=256,\n",
    "                                       layer_name=\"GraphConv\",\n",
    "                                       num_layers=3,\n",
    "                                       dp_rate_linear=0.5,\n",
    "                                       dp_rate=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train performance: 91.71%\n",
      "Test performance:  92.11%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train performance: {100.0*result['train']:4.2f}%\")\n",
    "print(f\"Test performance:  {100.0*result['test']:4.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1].https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html#Node-level-tasks:-Semi-supervised-node-classification\n",
    "\n",
    "[2].A Gentle Introduction to Graph Neural Networks, https://distill.pub/2021/gnn-intro/\n",
    "\n",
    "[3].Tutorial on networkx, https://networkx.org/documentation/stable/tutorial.html\n",
    "\n",
    "[4].PyG, https://pytorch-geometric.readthedocs.io/en/latest/\n",
    "\n",
    "[5]. GCN, https://mlabonne.github.io/blog/intrognn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
