{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62730f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/shared/apps/anaconda3/envs/torch190/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, metrics, preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f182e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5903c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ml-latest-small/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629bcc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info() # basically show schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407d5204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.userId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "484fcc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9724"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.movieId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad57b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    26818\n",
       "3.0    20047\n",
       "5.0    13211\n",
       "3.5    13136\n",
       "4.5     8551\n",
       "2.0     7551\n",
       "2.5     5550\n",
       "1.0     2811\n",
       "1.5     1791\n",
       "0.5     1370\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.value_counts() #check value distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29af0374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100836, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72486a1",
   "metadata": {},
   "source": [
    "#### Training Dataset Class Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456e8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset:\n",
    "    def __init__(self, users, movies, ratings):\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "    # len(movie_dataset)\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    # movie_dataset[1] \n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        users = self.users[item] \n",
    "        movies = self.movies[item]\n",
    "        ratings = self.ratings[item]\n",
    "        \n",
    "        return {\n",
    "            \"users\": torch.tensor(users, dtype=torch.long),\n",
    "            \"movies\": torch.tensor(movies, dtype=torch.long),\n",
    "            \"ratings\": torch.tensor(ratings, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33528c36",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d5e2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSysModel(nn.Module):\n",
    "    def __init__(self, n_users, n_movies):\n",
    "        super().__init__()\n",
    "        # trainable lookup matrix for shallow embedding vectors\n",
    "        \n",
    "        self.user_embed = nn.Embedding(n_users, 32)\n",
    "        self.movie_embed = nn.Embedding(n_movies, 32)\n",
    "        # user, movie embedding concat\n",
    "        self.out = nn.Linear(64, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, users, movies, ratings=None):\n",
    "        user_embeds = self.user_embed(users)\n",
    "        movie_embeds = self.movie_embed(movies)\n",
    "        output = torch.cat([user_embeds, movie_embeds], dim=1)\n",
    "        \n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e55691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the user and movie id to start from 0 so we don't run into index out of bound with Embedding\n",
    "lbl_user = preprocessing.LabelEncoder()\n",
    "lbl_movie = preprocessing.LabelEncoder()\n",
    "df.userId = lbl_user.fit_transform(df.userId.values)\n",
    "df.movieId = lbl_movie.fit_transform(df.movieId.values)\n",
    "\n",
    "df_train, df_valid = model_selection.train_test_split(\n",
    "    df, test_size=0.1, random_state=42, stratify=df.rating.values\n",
    ")\n",
    "\n",
    "train_dataset = MovieDataset(\n",
    "    users=df_train.userId.values,\n",
    "    movies=df_train.movieId.values,\n",
    "    ratings=df_train.rating.values\n",
    ")\n",
    "\n",
    "valid_dataset = MovieDataset(\n",
    "    users=df_valid.userId.values,\n",
    "    movies=df_valid.movieId.values,\n",
    "    ratings=df_valid.rating.values\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e30c79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n",
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'users': tensor([605,  83, 379, 452]), 'movies': tensor([3509,  658, 3094, 1303]), 'ratings': tensor([4, 3, 2, 4])}\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=4,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2) \n",
    "\n",
    "validation_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=4,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2) \n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "dataloader_data = dataiter.next() \n",
    "print(dataloader_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "810cac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecSysModel(\n",
    "    n_users=len(lbl_user.classes_),\n",
    "    n_movies=len(lbl_movie.classes_),\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())  \n",
    "sch = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
    "\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b0ce13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610\n",
      "9724\n",
      "9723\n",
      "90752\n"
     ]
    }
   ],
   "source": [
    "print(len(lbl_user.classes_))\n",
    "print(len(lbl_movie.classes_))\n",
    "print(df.movieId.max())\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c3820",
   "metadata": {},
   "source": [
    "#### Manually run a forward path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89b9af35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([605,  83, 379, 452])\n",
      "torch.Size([4])\n",
      "tensor([3509,  658, 3094, 1303])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(dataloader_data['users'])\n",
    "\n",
    "print(dataloader_data['users'].size())\n",
    "print(dataloader_data['movies'] )\n",
    "print(dataloader_data['movies'].size())\n",
    "\n",
    "user_embed = nn.Embedding(len(lbl_user.classes_), 32)\n",
    "movie_embed = nn.Embedding(len(lbl_movie.classes_), 32)\n",
    "\n",
    "out = nn.Linear(64, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e6f6d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_embeds torch.Size([4, 32])\n",
      "user_embeds tensor([[-0.2520, -0.9219,  1.4858, -1.6553, -1.6857, -0.6010, -0.8813,  2.3171,\n",
      "          1.1420, -0.5452,  0.6982,  1.1811, -0.4357, -0.3685,  0.2750, -0.0429,\n",
      "         -0.5116,  0.5405, -0.2553, -1.7710,  0.6795,  0.8117, -0.1782,  0.0936,\n",
      "          0.5063,  0.7133,  1.4532,  0.2172, -1.1686, -1.6555,  1.3895,  0.3345],\n",
      "        [-1.4396, -0.8579, -0.6026,  0.2032, -1.1084, -0.3011, -1.3868, -0.4079,\n",
      "         -1.1426, -0.8387,  0.4180, -0.5627, -0.3313,  1.6068,  0.0989, -2.1554,\n",
      "         -0.9992,  0.7265,  1.3826, -0.2183,  1.7908,  0.1678,  0.7581, -0.8694,\n",
      "         -0.4218,  0.0511,  0.1504,  0.5761,  2.4219,  1.2993,  2.2231, -1.5905],\n",
      "        [ 0.5163, -0.1067, -0.3334, -0.4875, -0.3143, -0.2787,  0.2424,  0.5763,\n",
      "          0.6580, -1.7541, -0.1250,  0.2865, -0.0250, -1.7641, -0.1478, -0.3840,\n",
      "         -0.3825, -2.6243,  0.1302, -0.9301, -0.3957,  0.5394,  0.6767,  0.6628,\n",
      "          0.7303,  0.1389, -0.7761, -0.7067,  1.5240, -1.1904,  0.1341,  0.5190],\n",
      "        [ 1.7720,  0.0225, -1.5171, -1.5490, -0.1463,  0.6564,  0.6968,  0.3112,\n",
      "          0.4695, -0.2891,  0.9779,  0.0040,  2.2994, -0.2047,  0.2118, -0.8225,\n",
      "          0.7092,  1.2519, -1.4831,  0.5801,  0.6332, -0.1575, -0.2379,  1.3076,\n",
      "         -0.0079,  0.1810, -2.0774, -1.6195,  0.5000, -1.2192,  0.7507,  0.9502]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "movie_embeds torch.Size([4, 32])\n",
      "movie_embeds tensor([[ 1.4746e+00, -1.3736e+00,  2.0782e-01, -1.5141e-02,  2.5132e-01,\n",
      "         -1.9066e+00,  1.0632e+00,  5.2773e-01,  1.6183e+00, -9.1351e-01,\n",
      "          1.9467e-01,  4.8739e-01, -1.1913e+00, -1.1012e+00, -1.2242e+00,\n",
      "         -3.9264e-01,  1.0330e+00, -1.3296e+00,  2.3285e+00, -7.1858e-02,\n",
      "          2.1191e-03,  1.3590e+00,  3.1541e-01,  6.9873e-01,  1.3637e+00,\n",
      "         -1.4589e+00,  5.0842e-01, -1.2153e+00, -3.8979e-01, -1.3256e+00,\n",
      "         -2.3512e+00,  1.9720e+00],\n",
      "        [-1.5915e+00, -1.0077e+00,  1.1817e+00,  9.2494e-01,  7.8497e-01,\n",
      "          4.9482e-01, -2.7140e+00,  1.9978e-01, -1.0351e+00,  1.7563e+00,\n",
      "         -1.5209e+00, -1.1812e+00, -9.9726e-01,  8.7247e-01,  5.3964e-01,\n",
      "         -1.6210e+00, -2.2053e-01, -7.1692e-02, -1.8808e-01,  1.2216e+00,\n",
      "          6.9330e-01, -4.1035e-01,  7.0723e-02,  1.2535e+00,  2.3215e-01,\n",
      "         -3.7688e-01,  3.9085e-01,  1.3882e+00, -6.7629e-01, -1.5306e+00,\n",
      "         -1.8271e+00, -2.3307e+00],\n",
      "        [-1.2148e+00,  6.4091e-01, -7.0276e-01,  5.6299e-01, -3.1684e-01,\n",
      "         -1.1358e+00,  1.3134e+00, -2.1831e-01, -1.5835e+00,  7.0548e-01,\n",
      "          6.3639e-02, -4.7468e-01, -1.4027e+00,  1.0676e+00, -2.5588e+00,\n",
      "         -2.0148e-01,  4.6085e-01, -1.1623e+00,  1.2146e+00, -4.4341e-01,\n",
      "         -3.3784e-01,  8.3472e-01,  1.8331e+00, -2.3721e+00,  4.5882e-01,\n",
      "          7.8114e-01, -1.7920e+00, -2.7014e-01,  1.1545e-01, -7.4832e-01,\n",
      "         -9.5157e-01, -1.3901e+00],\n",
      "        [ 2.1161e-01,  1.5862e-01, -2.4426e+00,  2.9322e+00, -8.5649e-01,\n",
      "         -9.1503e-02,  1.2741e+00,  8.8797e-01,  4.0176e-01, -4.1964e-01,\n",
      "         -7.8520e-01,  3.4232e-01, -1.0630e+00,  7.2926e-02,  1.0635e+00,\n",
      "          9.0719e-01, -2.8420e-01,  1.4421e+00,  8.9466e-01,  8.6616e-02,\n",
      "          8.3789e-01, -1.0829e+00, -7.0390e-01,  4.1110e-01, -6.6788e-02,\n",
      "          9.4431e-01,  3.2239e-01,  6.9617e-01,  7.2228e-01,  3.7028e-01,\n",
      "          5.4522e-01, -7.8421e-02]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "user_embeds = user_embed(dataloader_data['users'])\n",
    "movie_embeds = movie_embed(dataloader_data['movies'])\n",
    "print(f\"user_embeds {user_embeds.size()}\")\n",
    "print(f\"user_embeds {user_embeds}\")\n",
    "print(f\"movie_embeds {movie_embeds.size()}\")\n",
    "print(f\"movie_embeds {movie_embeds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18d20e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([4, 64])\n",
      "output: tensor([[-2.5205e-01, -9.2187e-01,  1.4858e+00, -1.6553e+00, -1.6857e+00,\n",
      "         -6.0103e-01, -8.8135e-01,  2.3171e+00,  1.1420e+00, -5.4518e-01,\n",
      "          6.9816e-01,  1.1811e+00, -4.3574e-01, -3.6850e-01,  2.7498e-01,\n",
      "         -4.2889e-02, -5.1164e-01,  5.4046e-01, -2.5533e-01, -1.7710e+00,\n",
      "          6.7952e-01,  8.1169e-01, -1.7821e-01,  9.3586e-02,  5.0630e-01,\n",
      "          7.1327e-01,  1.4532e+00,  2.1715e-01, -1.1686e+00, -1.6555e+00,\n",
      "          1.3895e+00,  3.3448e-01,  1.4746e+00, -1.3736e+00,  2.0782e-01,\n",
      "         -1.5141e-02,  2.5132e-01, -1.9066e+00,  1.0632e+00,  5.2773e-01,\n",
      "          1.6183e+00, -9.1351e-01,  1.9467e-01,  4.8739e-01, -1.1913e+00,\n",
      "         -1.1012e+00, -1.2242e+00, -3.9264e-01,  1.0330e+00, -1.3296e+00,\n",
      "          2.3285e+00, -7.1858e-02,  2.1191e-03,  1.3590e+00,  3.1541e-01,\n",
      "          6.9873e-01,  1.3637e+00, -1.4589e+00,  5.0842e-01, -1.2153e+00,\n",
      "         -3.8979e-01, -1.3256e+00, -2.3512e+00,  1.9720e+00],\n",
      "        [-1.4396e+00, -8.5791e-01, -6.0258e-01,  2.0323e-01, -1.1084e+00,\n",
      "         -3.0106e-01, -1.3868e+00, -4.0790e-01, -1.1426e+00, -8.3875e-01,\n",
      "          4.1795e-01, -5.6274e-01, -3.3129e-01,  1.6068e+00,  9.8878e-02,\n",
      "         -2.1554e+00, -9.9925e-01,  7.2653e-01,  1.3826e+00, -2.1826e-01,\n",
      "          1.7908e+00,  1.6780e-01,  7.5813e-01, -8.6939e-01, -4.2185e-01,\n",
      "          5.1069e-02,  1.5038e-01,  5.7607e-01,  2.4219e+00,  1.2993e+00,\n",
      "          2.2231e+00, -1.5905e+00, -1.5915e+00, -1.0077e+00,  1.1817e+00,\n",
      "          9.2494e-01,  7.8497e-01,  4.9482e-01, -2.7140e+00,  1.9978e-01,\n",
      "         -1.0351e+00,  1.7563e+00, -1.5209e+00, -1.1812e+00, -9.9726e-01,\n",
      "          8.7247e-01,  5.3964e-01, -1.6210e+00, -2.2053e-01, -7.1692e-02,\n",
      "         -1.8808e-01,  1.2216e+00,  6.9330e-01, -4.1035e-01,  7.0723e-02,\n",
      "          1.2535e+00,  2.3215e-01, -3.7688e-01,  3.9085e-01,  1.3882e+00,\n",
      "         -6.7629e-01, -1.5306e+00, -1.8271e+00, -2.3307e+00],\n",
      "        [ 5.1629e-01, -1.0671e-01, -3.3338e-01, -4.8747e-01, -3.1431e-01,\n",
      "         -2.7875e-01,  2.4235e-01,  5.7634e-01,  6.5800e-01, -1.7541e+00,\n",
      "         -1.2501e-01,  2.8654e-01, -2.5005e-02, -1.7641e+00, -1.4785e-01,\n",
      "         -3.8400e-01, -3.8252e-01, -2.6243e+00,  1.3017e-01, -9.3012e-01,\n",
      "         -3.9572e-01,  5.3938e-01,  6.7672e-01,  6.6282e-01,  7.3028e-01,\n",
      "          1.3893e-01, -7.7611e-01, -7.0667e-01,  1.5240e+00, -1.1904e+00,\n",
      "          1.3415e-01,  5.1900e-01, -1.2148e+00,  6.4091e-01, -7.0276e-01,\n",
      "          5.6299e-01, -3.1684e-01, -1.1358e+00,  1.3134e+00, -2.1831e-01,\n",
      "         -1.5835e+00,  7.0548e-01,  6.3639e-02, -4.7468e-01, -1.4027e+00,\n",
      "          1.0676e+00, -2.5588e+00, -2.0148e-01,  4.6085e-01, -1.1623e+00,\n",
      "          1.2146e+00, -4.4341e-01, -3.3784e-01,  8.3472e-01,  1.8331e+00,\n",
      "         -2.3721e+00,  4.5882e-01,  7.8114e-01, -1.7920e+00, -2.7014e-01,\n",
      "          1.1545e-01, -7.4832e-01, -9.5157e-01, -1.3901e+00],\n",
      "        [ 1.7720e+00,  2.2489e-02, -1.5171e+00, -1.5490e+00, -1.4633e-01,\n",
      "          6.5638e-01,  6.9676e-01,  3.1117e-01,  4.6949e-01, -2.8906e-01,\n",
      "          9.7789e-01,  3.9991e-03,  2.2994e+00, -2.0468e-01,  2.1177e-01,\n",
      "         -8.2253e-01,  7.0920e-01,  1.2519e+00, -1.4831e+00,  5.8009e-01,\n",
      "          6.3316e-01, -1.5747e-01, -2.3793e-01,  1.3076e+00, -7.8928e-03,\n",
      "          1.8096e-01, -2.0774e+00, -1.6195e+00,  4.9995e-01, -1.2192e+00,\n",
      "          7.5073e-01,  9.5022e-01,  2.1161e-01,  1.5862e-01, -2.4426e+00,\n",
      "          2.9322e+00, -8.5649e-01, -9.1503e-02,  1.2741e+00,  8.8797e-01,\n",
      "          4.0176e-01, -4.1964e-01, -7.8520e-01,  3.4232e-01, -1.0630e+00,\n",
      "          7.2926e-02,  1.0635e+00,  9.0719e-01, -2.8420e-01,  1.4421e+00,\n",
      "          8.9466e-01,  8.6616e-02,  8.3789e-01, -1.0829e+00, -7.0390e-01,\n",
      "          4.1110e-01, -6.6788e-02,  9.4431e-01,  3.2239e-01,  6.9617e-01,\n",
      "          7.2228e-01,  3.7028e-01,  5.4522e-01, -7.8421e-02]],\n",
      "       grad_fn=<CatBackward>)\n",
      "output: tensor([[-0.9189],\n",
      "        [ 0.0534],\n",
      "        [-0.2054],\n",
      "        [-0.2608]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = torch.cat([user_embeds, movie_embeds], dim=1) \n",
    "print(f\"output: {output.size()}\")\n",
    "print(f\"output: {output}\")\n",
    "output = out(output)\n",
    "print(f\"output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59d37dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_output: tensor([[-0.5564],\n",
      "        [ 0.6797],\n",
      "        [-0.2629],\n",
      "        [-0.5443]], device='cuda:0'), size: torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = model(dataloader_data['users'].to(device), \n",
    "                   dataloader_data[\"movies\"].to(device))\n",
    "\n",
    "    print(f\"model_output: {model_output}, size: {model_output.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85cc0014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 3, 2, 4])\n",
      "tensor([[4],\n",
      "        [3],\n",
      "        [2],\n",
      "        [4]])\n",
      "tensor([[-0.5564],\n",
      "        [ 0.6797],\n",
      "        [-0.2629],\n",
      "        [-0.5443]], device='cuda:0')\n",
      "tensor(13)\n",
      "tensor(-13.6840, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "rating = dataloader_data[\"ratings\"]\n",
    "print(rating)\n",
    "print(rating.view(4, -1))\n",
    "print(model_output)\n",
    "\n",
    "print(rating.sum())\n",
    "\n",
    "print(model_output.sum() - rating.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027b126",
   "metadata": {},
   "source": [
    "#### Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9be08a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n",
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss at step: 5000 is 0.5101097367048264\n",
      "epoch 0 loss at step: 10000 is 0.18411675513386727\n",
      "epoch 0 loss at step: 15000 is 0.11208086706530303\n",
      "epoch 0 loss at step: 20000 is 0.08568620623815805\n",
      "epoch 0 loss at step: 25000 is 0.0691491937073879\n",
      "epoch 0 loss at step: 30000 is 0.06542395735876634\n",
      "epoch 0 loss at step: 35000 is 0.0672071553981863\n",
      "epoch 0 loss at step: 40000 is 0.06361753194155172\n",
      "epoch 0 loss at step: 45000 is 0.06026220014132559\n",
      "epoch 0 loss at step: 50000 is 0.06088470030361787\n",
      "epoch 0 loss at step: 55000 is 0.06050228049024008\n",
      "epoch 0 loss at step: 60000 is 0.0552825854354538\n",
      "epoch 0 loss at step: 65000 is 0.06011045858934522\n",
      "epoch 0 loss at step: 70000 is 0.05844800607636571\n",
      "epoch 0 loss at step: 75000 is 0.059422189733479173\n",
      "epoch 0 loss at step: 80000 is 0.057488630441110584\n",
      "epoch 0 loss at step: 85000 is 0.05840645083812997\n",
      "epoch 0 loss at step: 90000 is 0.056306275022495536\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "total_loss = 0\n",
    "plot_steps, print_steps = 5000, 5000\n",
    "step_cnt = 0\n",
    "all_losses_list = [] \n",
    "\n",
    "model.train() \n",
    "for epoch_i in range(epochs):\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        output = model(train_data[\"users\"].to(device), \n",
    "                       train_data[\"movies\"].to(device)\n",
    "                      ) \n",
    "        \n",
    "        # .view(4, -1) is to reshape the rating to match the shape of model output which is 4x1\n",
    "        rating = train_data[\"ratings\"].view(4, -1).to(torch.float32).to(device)\n",
    "\n",
    "        loss = loss_func(output, rating)\n",
    "        total_loss = total_loss + loss.sum().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step_cnt = step_cnt + len(train_data[\"users\"])\n",
    "        \n",
    "\n",
    "        if(step_cnt % plot_steps == 0):\n",
    "            avg_loss = total_loss/(len(train_data[\"users\"]) * plot_steps)\n",
    "            print(f\"epoch {epoch_i} loss at step: {step_cnt} is {avg_loss}\")\n",
    "            all_losses_list.append(avg_loss)\n",
    "            total_loss = 0 # reset total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d213d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc078a",
   "metadata": {},
   "source": [
    "##### Evaluation with RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7882f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n",
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rms: 0.46449074555714354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model_output_list = []\n",
    "target_rating_list = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batched_data in enumerate(validation_loader): \n",
    "        model_output = model(batched_data['users'].to(device), \n",
    "                       batched_data[\"movies\"].to(device))\n",
    "        \n",
    "        model_output_list.append(model_output.sum().item() / len(batched_data['users']) )\n",
    "\n",
    "        target_rating = batched_data[\"ratings\"].to(device)\n",
    "        \n",
    "        target_rating_list.append(target_rating.sum().item() / len(batched_data['users']))\n",
    "\n",
    "#         print(f\"model_output: {model_output}, target_rating: {target_rating}\")\n",
    "\n",
    "\n",
    "# squared If True returns MSE value, if False returns RMSE value.\n",
    "rms = mean_squared_error(target_rating_list, model_output_list, squared=False)\n",
    "print(f\"rms: {rms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28075f9",
   "metadata": {},
   "source": [
    "##### Evaluation with Recall@K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e719617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n",
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# a dict that stores a list of predicted rating and actual rating pair for each user \n",
    "user_est_true = defaultdict(list)\n",
    "\n",
    "# iterate through the validation data to build the user-> [(y1, y1_hat), (y2, y2_hat)...]   \n",
    "with torch.no_grad():\n",
    "    for i, batched_data in enumerate(validation_loader): \n",
    "        users = batched_data['users']\n",
    "        movies = batched_data['movies']\n",
    "        ratings = batched_data['ratings']\n",
    "        \n",
    "        model_output = model(batched_data['users'].to(device), batched_data[\"movies\"].to(device))\n",
    "\n",
    "        for i in range(len(users)):\n",
    "            user_id = users[i].item()\n",
    "            movie_id = movies[i].item() \n",
    "            pred_rating = model_output[i][0].item()\n",
    "            true_rating = ratings[i].item()\n",
    "            \n",
    "#             print(f\"{user_id}, {movie_id}, {pred_rating}, {true_rating}\")\n",
    "            user_est_true[user_id].append((pred_rating, true_rating))            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df848cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03b4ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "\n",
    "    k=10\n",
    "    threshold=3.5\n",
    "\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value. \n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # 这里 get the number of actual relevant item\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # 这里 get the number of recommended item that are predicted relevent and within topk\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # 这里 get the number of recommented item that' is also actually relevant within topk\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "#         print(f\"uid {uid},  n_rel {n_rel}, n_rec_k {n_rec_k}, n_rel_and_rec_k {n_rel_and_rec_k}\")\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e97cc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision @ 10: 0.5761474867724871\n",
      "recall @ 10 : 0.4926423547682639\n"
     ]
    }
   ],
   "source": [
    "# Precision and recall can then be averaged over all users\n",
    "print(f\"precision @ {k}: {sum(prec for prec in precisions.values()) / len(precisions)}\")\n",
    "\n",
    "print(f\"recall @ {k} : {sum(rec for rec in recalls.values()) / len(recalls)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d77840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab2791a1",
   "metadata": {},
   "source": [
    "#### What if we remove the fully connected layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2817276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSysModel2(nn.Module):\n",
    "    def __init__(self, n_users, n_movies):\n",
    "        super().__init__()\n",
    "        # trainable lookup matrix for shallow embedding vectors\n",
    "        \n",
    "        self.user_embed = nn.Embedding(n_users, 32)\n",
    "        self.movie_embed = nn.Embedding(n_movies, 32)\n",
    "\n",
    "    \n",
    "    def forward(self, users, movies, ratings=None):\n",
    "        user_embeds = self.user_embed(users)\n",
    "        movie_embeds = self.movie_embed(movies)\n",
    "        output = torch.matmul(user_embeds, movie_embeds.T)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7a5b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecSysModel2(\n",
    "    n_users=len(lbl_user.classes_),\n",
    "    n_movies=len(lbl_movie.classes_),\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())  \n",
    "sch = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
    "\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc6d992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n",
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n",
      "/workspace/shared/apps/anaconda3/envs/torch190/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([4, 1])) that is different to the input size (torch.Size([4, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss at step: 5000 is 2.67913779668808\n",
      "epoch 0 loss at step: 10000 is 2.512531473302841\n",
      "epoch 0 loss at step: 15000 is 2.371936664915085\n",
      "epoch 0 loss at step: 20000 is 2.178830087709427\n",
      "epoch 0 loss at step: 25000 is 2.0617033567905425\n",
      "epoch 0 loss at step: 30000 is 1.9494199245452881\n",
      "epoch 0 loss at step: 35000 is 1.8298495384693145\n",
      "epoch 0 loss at step: 40000 is 1.7433323801279068\n",
      "epoch 0 loss at step: 45000 is 1.6428947719812392\n",
      "epoch 0 loss at step: 50000 is 1.5580266164779664\n",
      "epoch 0 loss at step: 55000 is 1.521391928434372\n",
      "epoch 0 loss at step: 60000 is 1.423901296043396\n",
      "epoch 0 loss at step: 65000 is 1.3849665390253068\n",
      "epoch 0 loss at step: 70000 is 1.3135926964521407\n",
      "epoch 0 loss at step: 75000 is 1.3041538632631302\n",
      "epoch 0 loss at step: 80000 is 1.2365363868117332\n",
      "epoch 0 loss at step: 85000 is 1.188675112247467\n",
      "epoch 0 loss at step: 90000 is 1.1474454346895218\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "total_loss = 0\n",
    "plot_steps, print_steps = 5000, 5000\n",
    "step_cnt = 0\n",
    "all_losses_list = [] \n",
    "\n",
    "model.train() \n",
    "for epoch_i in range(epochs):\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        output = model(train_data[\"users\"].to(device), \n",
    "                       train_data[\"movies\"].to(device)\n",
    "                      ) \n",
    "        \n",
    "        # .view(4, -1) is to reshape the rating to match the shape of model output which is 4x1\n",
    "        rating = train_data[\"ratings\"].view(4, -1).to(torch.float32).to(device)\n",
    "\n",
    "        loss = loss_func(output, rating)\n",
    "        total_loss = total_loss + loss.sum().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step_cnt = step_cnt + len(train_data[\"users\"])\n",
    "        \n",
    "\n",
    "        if(step_cnt % plot_steps == 0):\n",
    "            avg_loss = total_loss/(len(train_data[\"users\"]) * plot_steps)\n",
    "            print(f\"epoch {epoch_i} loss at step: {step_cnt} is {avg_loss}\")\n",
    "            all_losses_list.append(avg_loss)\n",
    "            total_loss = 0 # reset total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59494282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n",
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rms: 3.7906492731639703\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model_output_list = []\n",
    "target_rating_list = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batched_data in enumerate(validation_loader): \n",
    "        model_output = model(batched_data['users'].to(device), \n",
    "                       batched_data[\"movies\"].to(device))\n",
    "        \n",
    "        model_output_list.append(model_output.sum().item() / len(batched_data['users']) )\n",
    "\n",
    "        target_rating = batched_data[\"ratings\"].to(device)\n",
    "        \n",
    "        target_rating_list.append(target_rating.sum().item() / len(batched_data['users']))\n",
    "\n",
    "#         print(f\"model_output: {model_output}, target_rating: {target_rating}\")\n",
    "\n",
    "\n",
    "# squared If True returns MSE value, if False returns RMSE value.\n",
    "rms = mean_squared_error(target_rating_list, model_output_list, squared=False)\n",
    "print(f\"rms: {rms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8c8c50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n",
      "/tmp/ipykernel_1573079/3366589054.py:19: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  \"ratings\": torch.tensor(ratings, dtype=torch.long),\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# a dict that stores a list of predicted rating and actual rating pair for each user \n",
    "user_est_true = defaultdict(list)\n",
    "\n",
    "# iterate through the validation data to build the user-> [(y1, y1_hat), (y2, y2_hat)...]   \n",
    "with torch.no_grad():\n",
    "    for i, batched_data in enumerate(validation_loader): \n",
    "        users = batched_data['users']\n",
    "        movies = batched_data['movies']\n",
    "        ratings = batched_data['ratings']\n",
    "        \n",
    "        model_output = model(batched_data['users'].to(device), batched_data[\"movies\"].to(device))\n",
    "\n",
    "        for i in range(len(users)):\n",
    "            user_id = users[i].item()\n",
    "            movie_id = movies[i].item() \n",
    "            pred_rating = model_output[i][0].item()\n",
    "            true_rating = ratings[i].item()\n",
    "            \n",
    "#             print(f\"{user_id}, {movie_id}, {pred_rating}, {true_rating}\")\n",
    "            user_est_true[user_id].append((pred_rating, true_rating))            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36cc1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "\n",
    "    k=10\n",
    "    threshold=3.5\n",
    "\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value. \n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # 这里 get the number of actual relevant item\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # 这里 get the number of recommended item that are predicted relevent and within topk\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # 这里 get the number of recommented item that' is also actually relevant within topk\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "#         print(f\"uid {uid},  n_rel {n_rel}, n_rec_k {n_rec_k}, n_rel_and_rec_k {n_rel_and_rec_k}\")\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edd09e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision @ 10: 0.35711772486772486\n",
      "recall @ 10 : 0.12584485741274626\n"
     ]
    }
   ],
   "source": [
    "# Precision and recall can then be averaged over all users\n",
    "print(f\"precision @ {k}: {sum(prec for prec in precisions.values()) / len(precisions)}\")\n",
    "\n",
    "print(f\"recall @ {k} : {sum(rec for rec in recalls.values()) / len(recalls)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f98e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
